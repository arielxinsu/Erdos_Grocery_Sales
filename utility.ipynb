{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a314c5",
   "metadata": {
    "papermill": {
     "duration": 0.005658,
     "end_time": "2024-10-23T21:10:59.083971",
     "exception": false,
     "start_time": "2024-10-23T21:10:59.078313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utility!\n",
    "\n",
    "A colleciton of a useful funcitons that I've found on Kaggle over the years or wrote myself to avoid future work and keep my notebooks a bit more readable.\n",
    "\n",
    "If it was something I found on kaggle, I put credits in each cell of where I found them... LMK if I have the original authors wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c76ab3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-23T21:10:59.094363Z",
     "iopub.status.busy": "2024-10-23T21:10:59.093978Z",
     "iopub.status.idle": "2024-10-23T21:11:00.450821Z",
     "shell.execute_reply": "2024-10-23T21:11:00.449865Z"
    },
    "papermill": {
     "duration": 1.365231,
     "end_time": "2024-10-23T21:11:00.453673",
     "exception": false,
     "start_time": "2024-10-23T21:10:59.088442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from IPython.display import display_html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ec2178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.464506Z",
     "iopub.status.busy": "2024-10-23T21:11:00.463647Z",
     "iopub.status.idle": "2024-10-23T21:11:00.478989Z",
     "shell.execute_reply": "2024-10-23T21:11:00.478063Z"
    },
    "papermill": {
     "duration": 0.02309,
     "end_time": "2024-10-23T21:11:00.481216",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.458126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_mean_encodings(train, target, categorical_columns, exclude_columns=None):\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []  # Initialize an empty list if no columns are to be excluded\n",
    "\n",
    "    encoding_list = []  # Use a list to collect all DataFrames    \n",
    "    \n",
    "    # Loop through categorical columns and calculate mean target per category\n",
    "    for col in categorical_columns:\n",
    "        if col not in exclude_columns:  # Skip the column if it's in the exclude list\n",
    "            # Calculate the mean target per category, treating NaNs as a separate category\n",
    "            category_means = train.groupby(col, dropna=False)[target].mean().reset_index()\n",
    "            # Rename columns for clarity\n",
    "            category_means.rename(columns={col: 'category', target: 'mean_target'}, inplace=True)\n",
    "            # Add a column to identify the original column name\n",
    "            category_means['col_name'] = col\n",
    "            # Adjust columns order\n",
    "            category_means = category_means[['col_name', 'category', 'mean_target']]\n",
    "            encoding_list.append(category_means)\n",
    "    \n",
    "    # Concatenate all into a single DataFrame\n",
    "    if encoding_list:\n",
    "        encodings = pd.concat(encoding_list, ignore_index=True)\n",
    "    else:\n",
    "        encodings = pd.DataFrame(columns=['col_name', 'category', 'mean_target'])  # Return empty DataFrame if no encodings\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def apply_encodings_and_replace(data, encodings):\n",
    "    \"\"\"\n",
    "    Applies mean target encodings to the categorical columns of the dataset and replaces the original\n",
    "    columns with the encoded ones by creating new columns and then dropping the old ones.\n",
    "    \n",
    "    Args:\n",
    "    data (DataFrame): The dataset to which encodings should be applied.\n",
    "    encodings (DataFrame): DataFrame containing the encodings, expected to have 'col_name', 'category', and 'mean_target' columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The dataset with the original categorical columns replaced by their encoded values.\n",
    "    \"\"\"\n",
    "    encoded_data = data.copy()\n",
    "    for col in data.columns:\n",
    "        if col in encodings['col_name'].unique():\n",
    "            # Create a new column for the encoded values\n",
    "            encoded_column_name = col + '_encoded'\n",
    "            encoded_data[encoded_column_name] = np.nan\n",
    "            # Fetch the relevant encodings for this column\n",
    "            column_encodings = encodings[encodings['col_name'] == col]\n",
    "            for _, row in column_encodings.iterrows():\n",
    "                category = row['category']\n",
    "                mean_target = row['mean_target']\n",
    "                # Assign the mean target value to the new column wherever the categories match\n",
    "                encoded_data.loc[encoded_data[col] == category, encoded_column_name] = mean_target\n",
    "            \n",
    "            # Optionally handle categories not found in the encodings (unseen categories)\n",
    "            if encoded_data[encoded_column_name].isna().any():\n",
    "                # Fill NaNs with the overall target mean as calculated in the def above\n",
    "                encoded_data[encoded_column_name].fillna(encodings.iat[-1, -1], inplace=True)\n",
    "\n",
    "            # Drop the original column after encoding is complete\n",
    "            encoded_data.drop(col, axis=1, inplace=True)\n",
    "            # Optionally rename the encoded column to the original name if needed\n",
    "            encoded_data.rename(columns={encoded_column_name: col}, inplace=True)\n",
    "\n",
    "    return encoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043a20bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.491331Z",
     "iopub.status.busy": "2024-10-23T21:11:00.490776Z",
     "iopub.status.idle": "2024-10-23T21:11:00.501573Z",
     "shell.execute_reply": "2024-10-23T21:11:00.500627Z"
    },
    "papermill": {
     "duration": 0.018977,
     "end_time": "2024-10-23T21:11:00.504459",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.485482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_disk_usage = get_disk_usage(f'{ROOT}/csv_files/train').reset_index()\\ntest_disk_usage = get_disk_usage(f'{ROOT}/csv_files/test')\\n\\ntrain_disk_usage.reset_index().merge(test_disk_usage, on=['file_name'],\\n                                     how='outer', suffixes=['_train', '_test'])                                     .sort_values(by='index').drop(columns=['index'])\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of CSV/parquet/etc in a directory\n",
    "def get_disk_usage(directory):\n",
    "    cmd = f'du {directory}/* -h | sort -rh'\n",
    "    result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "    output_lines = result.stdout.split('\\n')\n",
    "\n",
    "    # Extract file/directory names and sizes\n",
    "    data = [line.split('\\t') for line in output_lines if line]\n",
    "    df = pd.DataFrame(data, columns=['size', 'path'])\n",
    "    df['file_name'] = df.path.str.replace('train_|test_', '', regex=True).\\\n",
    "    apply(lambda x: Path(x).stem)\n",
    "    return df\n",
    "'''\n",
    "train_disk_usage = get_disk_usage(f'{ROOT}/csv_files/train').reset_index()\n",
    "test_disk_usage = get_disk_usage(f'{ROOT}/csv_files/test')\n",
    "\n",
    "train_disk_usage.reset_index().merge(test_disk_usage, on=['file_name'],\n",
    "                                     how='outer', suffixes=['_train', '_test'])\\\n",
    "                                     .sort_values(by='index').drop(columns=['index'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3577eac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.515651Z",
     "iopub.status.busy": "2024-10-23T21:11:00.515040Z",
     "iopub.status.idle": "2024-10-23T21:11:00.524896Z",
     "shell.execute_reply": "2024-10-23T21:11:00.523882Z"
    },
    "papermill": {
     "duration": 0.017691,
     "end_time": "2024-10-23T21:11:00.526997",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.509306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/darynarr/enefit-online-training/notebook\n",
    "\n",
    "# feature forward selection\n",
    "# change params to take in a model, and maybe a DF\n",
    "def select_columns(is_consumption = True):\n",
    "    selected_columns = pd.Series({\"year\": 1e5, \"month\": 1e5, \"day\":1e5, 'hour':1e5})\n",
    "    \n",
    "    while True:\n",
    "        n_selected = len(selected_columns)\n",
    "        current_loss = selected_columns.min()\n",
    "        for col in X.columns:\n",
    "            if col in selected_columns.index:\n",
    "                continue\n",
    "            cols = selected_columns.index.tolist() + [col]\n",
    "            \n",
    "            res = cross_validate(\n",
    "                # rewrite this for imput of model\n",
    "                estimator=get_model(\n",
    "                    params['consumption'] if is_consumption else params['production'], \n",
    "                    verbose=False, cat_cols = [c for c in cat_cols if c in cols]),\n",
    "                X=X[X['is_consumption']==int(is_consumption)][cols],\n",
    "                y=y[X['is_consumption']==int(is_consumption)],\n",
    "                scoring=\"neg_mean_absolute_error\",\n",
    "                cv= cv,\n",
    "                return_estimator=True\n",
    "            )\n",
    "            current_loss = -res['test_score'].mean()\n",
    "            \n",
    "            if current_loss < selected_columns.min():\n",
    "                selected_columns[col] = current_loss\n",
    "                logger.info(f\"{len(selected_columns)}, {col}, {current_loss}\")\n",
    "        if n_selected == len(selected_columns):\n",
    "            break\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca3ed7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.537087Z",
     "iopub.status.busy": "2024-10-23T21:11:00.536528Z",
     "iopub.status.idle": "2024-10-23T21:11:00.545863Z",
     "shell.execute_reply": "2024-10-23T21:11:00.544881Z"
    },
    "papermill": {
     "duration": 0.016801,
     "end_time": "2024-10-23T21:11:00.548117",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.531316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/kimtaehun/breif-eda-and-xgb-baseline-with-full-dataset\n",
    "def summary(df):\n",
    "    print(f'data shape: {df.shape}')\n",
    "    summ = pd.DataFrame(df.dtypes, columns=['data type'])\n",
    "    summ['#missing'] = df.isnull().sum().values \n",
    "    summ['%missing'] = df.isnull().sum().values / len(df) * 100\n",
    "    summ['#unique'] = df.nunique().values\n",
    "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
    "\n",
    "    # Debug: print the columns of desc\n",
    "    # print(\"Columns in desc:\", desc.columns)\n",
    "    \n",
    "    if 'min' in desc.columns:\n",
    "        summ['min'] = desc['min'].values\n",
    "    else:\n",
    "        summ['min'] = 'N/A'\n",
    "\n",
    "    if 'max' in desc.columns:\n",
    "        summ['max'] = desc['max'].values\n",
    "    else:\n",
    "        summ['max'] = 'N/A'\n",
    "    \n",
    "    summ['first value'] = df.iloc[0].values if len(df) > 0 else 'N/A'\n",
    "    summ['second value'] = df.iloc[1].values if len(df) > 1 else 'N/A'\n",
    "    summ['third value'] = df.iloc[2].values if len(df) > 2 else 'N/A'    \n",
    "\n",
    "#    return summ\n",
    "    display_html(summ)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831ed91f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.558332Z",
     "iopub.status.busy": "2024-10-23T21:11:00.557800Z",
     "iopub.status.idle": "2024-10-23T21:11:00.564882Z",
     "shell.execute_reply": "2024-10-23T21:11:00.563901Z"
    },
    "papermill": {
     "duration": 0.01455,
     "end_time": "2024-10-23T21:11:00.567012",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.552462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From chatGPT and then me editng it\n",
    "def ohe_categorical(df, categorical_cols, threshold=10, keep_col = True):\n",
    "    \"\"\"\n",
    "    One-hot encodes the categorical columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        categorical_cols (list): List of column names to one-hot encode.\n",
    "        threshold (int): The threshold for the number of unique values\n",
    "                         in a column to decide whether to one-hot encode it.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with one-hot encoded columns.\n",
    "        list: List of columns that weren't one-hot encoded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the columns that weren't one-hot encoded\n",
    "    not_encoded_cols = []\n",
    "\n",
    "    # Iterate through the categorical columns\n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count <= threshold:\n",
    "            # Perform one-hot encoding for columns with unique_count less than or equal to the threshold\n",
    "            if keep_col: copied_col = df[col]\n",
    "            df = pd.get_dummies(df, columns=[col])\n",
    "            if keep_col: df[col] = copied_col\n",
    "            \n",
    "        else:\n",
    "            # Append the column name to the not_encoded_cols list\n",
    "            not_encoded_cols.append(col)\n",
    "\n",
    "    return df, not_encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633dc5e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.577255Z",
     "iopub.status.busy": "2024-10-23T21:11:00.576516Z",
     "iopub.status.idle": "2024-10-23T21:11:00.581744Z",
     "shell.execute_reply": "2024-10-23T21:11:00.580744Z"
    },
    "papermill": {
     "duration": 0.012618,
     "end_time": "2024-10-23T21:11:00.584006",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.571388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_columns_by_type(df, data_type):\n",
    "    columns_list = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == data_type:\n",
    "            columns_list.append(column)\n",
    "    \n",
    "    return columns_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f68db1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.594118Z",
     "iopub.status.busy": "2024-10-23T21:11:00.593353Z",
     "iopub.status.idle": "2024-10-23T21:11:00.602384Z",
     "shell.execute_reply": "2024-10-23T21:11:00.601343Z"
    },
    "papermill": {
     "duration": 0.016588,
     "end_time": "2024-10-23T21:11:00.604792",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.588204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/andradaolteanu/rsna-fracture-detection-dicom-images-explore\n",
    "def df_info(df, name=\"Default\"): \n",
    "    print(clr.S+f\"=== {name} ===\"+clr.E)\n",
    "\n",
    "    if not hasattr(df, 'shape'):\n",
    "        print(clr.S + \"Shape:\" + clr.E, format(df.shape[0], \",\"), format(df.shape[1], \",\"))\n",
    "    else:\n",
    "        print(clr.S + \"Shape:\" + clr.E, df.shape)\n",
    "\n",
    "    print(clr.S+f\"Missing Values:\"+clr.E, format(df.isna().sum().sum(), \",\"), \"total missing datapoints.\")\n",
    "    print(clr.S+\"Columns:\"+clr.E, list(df.columns), \"\\n\")\n",
    "    \n",
    "    display_html(df.tail())\n",
    "    print(\"\\n\")\n",
    "\n",
    "class clr:\n",
    "    S = '\\033[1m' + '\\033[94m'\n",
    "    E = '\\033[0m'\n",
    "    \n",
    "my_colors = [\"#5EAFD9\", \"#449DD1\", \"#3977BB\", \n",
    "             \"#2D51A5\", \"#5C4C8F\", \"#8B4679\",\n",
    "             \"#C53D4C\", \"#E23836\", \"#FF4633\", \"#FF5746\"]\n",
    "CMAP1 = ListedColormap(my_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb27549b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.615538Z",
     "iopub.status.busy": "2024-10-23T21:11:00.614838Z",
     "iopub.status.idle": "2024-10-23T21:11:00.623686Z",
     "shell.execute_reply": "2024-10-23T21:11:00.622649Z"
    },
    "papermill": {
     "duration": 0.016807,
     "end_time": "2024-10-23T21:11:00.626028",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.609221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_and_display_csv(directory, date_field):\n",
    "    for dirname, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                print(os.path.join(dirname, filename))\n",
    "                no_ext = f'{os.path.splitext(filename)[0]}_df'\n",
    "                no_ext = no_ext.replace(\" \", \"_\")\n",
    "                no_ext = no_ext.replace(\"-\", \"_\")\n",
    "                \n",
    "                # Read the CSV headers first to check for the date_field\n",
    "                temp_df = pd.read_csv(os.path.join(dirname, filename), nrows=0)\n",
    "                if date_field in temp_df.columns:\n",
    "                    # Parse dates if the date_field is present, but don't set it as the index\n",
    "                    globals()[no_ext] = pd.read_csv(\n",
    "                        os.path.join(dirname, filename), \n",
    "                        parse_dates=[date_field],\n",
    "                        index_col=False  # Ensure the date field is not set as the index\n",
    "                    )\n",
    "                else:\n",
    "                    # Read normally without parsing dates\n",
    "                    globals()[no_ext] = pd.read_csv(os.path.join(dirname, filename), index_col=False)\n",
    "                del temp_df\n",
    "                df_info(globals()[no_ext], no_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f0bb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.636726Z",
     "iopub.status.busy": "2024-10-23T21:11:00.635772Z",
     "iopub.status.idle": "2024-10-23T21:11:00.643055Z",
     "shell.execute_reply": "2024-10-23T21:11:00.641941Z"
    },
    "papermill": {
     "duration": 0.015064,
     "end_time": "2024-10-23T21:11:00.645502",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.630438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These helper and data cleaning functions are from the old fast.ai course\n",
    "# The repository is here: https://github.com/fastai/fastai/tree/master/old\n",
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)\n",
    "        \n",
    "def make_date(df, date_field:str):\n",
    "    \"Make sure `df[field_name]` is of the right date type.\"\n",
    "    field_dtype = df[date_field].dtype\n",
    "    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        field_dtype = np.datetime64\n",
    "    if not np.issubdtype(field_dtype, np.datetime64):\n",
    "        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae6482ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.656252Z",
     "iopub.status.busy": "2024-10-23T21:11:00.655657Z",
     "iopub.status.idle": "2024-10-23T21:11:00.669817Z",
     "shell.execute_reply": "2024-10-23T21:11:00.668772Z"
    },
    "papermill": {
     "duration": 0.022289,
     "end_time": "2024-10-23T21:11:00.672318",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.650029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\", sin_cos=False, exclude_cols=[]):\n",
    "    \"\"\"\n",
    "    Add Date Parts converts a column of df from a datetime64 to many columns containing \n",
    "    the information from the date. It returns a modified version of the original DataFrame.\n",
    "    If sin_cos is True, sine and cosine features for the relevant datetime components\n",
    "    are also added.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the date column.\n",
    "    fldnames (str or list): The name(s) of the date column(s).\n",
    "    drop (bool): Whether to drop the original date column(s).\n",
    "    time (bool): Whether to include time-related date parts (Hour, Minute, Second).\n",
    "    errors (str): How to handle parsing errors when converting to datetime.\n",
    "    sin_cos (bool): Whether to add sine and cosine features for date components.\n",
    "    exclude_cols (list): List of columns to exclude from date part creation.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with added date parts.\n",
    "    \"\"\"\n",
    "\n",
    "    def create_sine_cosine_features(df, column, max_value):\n",
    "        if column in df.columns:\n",
    "            df[f'{column}_sin'] = np.sin(2 * np.pi * df[column] / max_value)\n",
    "            df[f'{column}_cos'] = np.cos(2 * np.pi * df[column] / max_value)\n",
    "        return df\n",
    "\n",
    "    if isinstance(fldnames, str):\n",
    "        fldnames = [fldnames]\n",
    "    \n",
    "    for fldname in fldnames:\n",
    "        if fldname in exclude_cols:\n",
    "            continue  # Skip this column if it's in the exclude list\n",
    "            \n",
    "        fld = df[fldname]\n",
    "        fld_dtype = fld.dtype\n",
    "        \n",
    "        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            fld_dtype = np.datetime64\n",
    "\n",
    "        if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "        \n",
    "        targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "        attr = ['Year', 'Month', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', \n",
    "                'Is_year_end', 'Is_year_start']\n",
    "        \n",
    "        if time:\n",
    "            attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        \n",
    "        for n in attr:\n",
    "            df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "        \n",
    "        if drop:\n",
    "            df = df.drop(fldname, axis=1)\n",
    "\n",
    "        if sin_cos:\n",
    "            # Sine and Cosine transformations\n",
    "            df = create_sine_cosine_features(df, targ_pre + 'Hour', 24)\n",
    "            df = create_sine_cosine_features(df, targ_pre + 'Day', 30)  # Approximation\n",
    "            df = create_sine_cosine_features(df, targ_pre + 'Dayofweek', 7)\n",
    "            df = create_sine_cosine_features(df, targ_pre + 'Dayofyear', 365)  # May need adjustment for leap years\n",
    "            df = create_sine_cosine_features(df, targ_pre + 'Month', 12)\n",
    "            # Add additional sine and cosine features for Minute and Second if required\n",
    "            if time:\n",
    "                df = create_sine_cosine_features(df, targ_pre + 'Minute', 60)\n",
    "                df = create_sine_cosine_features(df, targ_pre + 'Second', 60)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d499093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T21:11:00.683721Z",
     "iopub.status.busy": "2024-10-23T21:11:00.682803Z",
     "iopub.status.idle": "2024-10-23T21:11:00.696097Z",
     "shell.execute_reply": "2024-10-23T21:11:00.695017Z"
    },
    "papermill": {
     "duration": 0.021438,
     "end_time": "2024-10-23T21:11:00.698508",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.677070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40570781",
   "metadata": {
    "papermill": {
     "duration": 0.004303,
     "end_time": "2024-10-23T21:11:00.707541",
     "exception": false,
     "start_time": "2024-10-23T21:11:00.703238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30458,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.579393,
   "end_time": "2024-10-23T21:11:01.433322",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T21:10:48.853929",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
