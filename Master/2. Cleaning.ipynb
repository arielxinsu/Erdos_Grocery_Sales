{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "stores = pd.read_csv('../data/stores.csv')\n",
    "transactions = pd.read_csv('../data/transactions.csv')\n",
    "oil = pd.read_csv('../data/oil.csv')\n",
    "holidays = pd.read_csv('../data/holidays_events.csv')\n",
    "\n",
    "# Dictionary for all datasets\n",
    "datasets = {'train':train, 'test':test, 'stores':stores, 'transactions':transactions, 'oil':oil, 'holidays':holidays}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarizing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert any dates in the datasets to pandas Timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in iter(datasets.values()):\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime( df['date'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `id` column from the training set. The preliminary analysis proved this is just a redundant row indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oil Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preliminary analysis shows 43 missing daily oil price values and missing weekend oil price values. \n",
    "\n",
    "We add the weekend days first as more null values, then interpolate \n",
    "\n",
    "Also, the oil price for the very first day (2013-01-01) is missing. We can manually add that here using the oil prices from 2012-12-31 and 2013-01-02 from (https://fred.stlouisfed.org/data/DCOILWTICO). We separately verified that is safe to do, since these oil prices match the ones in the oil dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually add oil price for the first day using average of 2012-12-31 and 2013-01-02 oil prices\n",
    "oil.iloc[0,1] = 92.485                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with all dates in desired range, including weekends\n",
    "dates = pd.DataFrame(pd.date_range(start='1/1/2013', end='8/31/2017',freq='D'), columns=['date'])\n",
    "# Merge with oil data set, so that weekend dates are added to oil with null values\n",
    "oil = dates.merge(oil,how='left', on='date')\n",
    "# Interpolate all missing values in oil (all but possibly one of the gaps are of size 1,2, or 3)\n",
    "oil['dcoilwtico'] = oil['dcoilwtico'].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holiday Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preliminary analysis, two transferred holidays need their description updated so that all tranfer holidays have consistent formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.loc[304,'description'] = 'Traslado Fundacion de Cuenca'\n",
    "holidays.loc[329, 'description'] = 'Traslado Fundacion de Ibarra'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were also a couple mislabelled Additional Holidays, and one that should be deleted for redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                2016-07-24 00:00:00\n",
       "type                         Additional\n",
       "locale                            Local\n",
       "locale_name                   Guayaquil\n",
       "description    Fundacion de Guayaquil-1\n",
       "transferred                       False\n",
       "Name: 264, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holidays.loc[264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.loc[182,'type'] = 'Additional'\n",
    "holidays.loc[322,'type'] = 'Holiday'\n",
    "holidays = holidays.drop(264, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first inner join the store and transaction data along the `store_nbr` column.\n",
    "\n",
    "This guarantees no duplicates or new null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge stores with transactions on date and store_nbr\n",
    "X = stores.merge(transactions, how='inner', on='store_nbr')\n",
    "X = X.sort_values(by=['date','store_nbr'],axis=0).reset_index(drop=True)\n",
    "X = X[['date','store_nbr','type','cluster','city','state','transactions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging more data, we break down the `date` column into year, month, week, day, and day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(**{'year': pd.Series( [X.date[i].year for i in X.index]), \n",
    "            'month': pd.Series( [X.date[i].month for i in X.index]), \n",
    "            'week_number': pd.Series( [X.date[i].week for i in X.index]), \n",
    "            'day':pd.Series( [X.date[i].day for i in X.index]), \n",
    "            'day_of_week': pd.Series( [X.date[i].dayofweek for i in X.index]) })\n",
    "X = X[['date','year', 'month', 'week_number', 'day', 'day_of_week','store_nbr','type','cluster','city','state','transactions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inner join oil prices along the `date` column.\n",
    "\n",
    "Since there is one oil price per date and we filled null values in oil, this won't give duplicates or new null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.merge(oil, how='left', on='date')\n",
    "X = X.sort_values(by=['date','store_nbr'],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we join the holiday data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we inner join our training data along both `date` and `store_nbr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.merge(train, how='inner', on=['date','store_nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"../data/merged_train.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
